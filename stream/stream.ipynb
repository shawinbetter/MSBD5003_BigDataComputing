{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "lines.pprint()\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(20)  # Wait for the computation to terminate\n",
    "ssc.stop(stopSparkContext=False)  # Stop the StreamingContext without stopping the SparkContext\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-12-07 15:42:55\n",
      "-------------------------------------------\n",
      "('other', 15486)\n",
      "('first', 10815)\n",
      "('many', 9773)\n",
      "('new', 6272)\n",
      "('system', 5063)\n",
      "('american', 4744)\n",
      "('several', 4545)\n",
      "('century', 4492)\n",
      "('same', 4394)\n",
      "('=', 4313)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-12-07 15:43:00\n",
      "-------------------------------------------\n",
      "('other', 15319)\n",
      "('first', 10709)\n",
      "('many', 9575)\n",
      "('new', 6242)\n",
      "('system', 5111)\n",
      "('american', 4777)\n",
      "('century', 4538)\n",
      "('several', 4515)\n",
      "('same', 4453)\n",
      "('=', 4361)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-12-07 15:43:05\n",
      "-------------------------------------------\n",
      "('other', 15346)\n",
      "('first', 10517)\n",
      "('many', 9706)\n",
      "('new', 6218)\n",
      "('system', 5266)\n",
      "('american', 4940)\n",
      "('several', 4615)\n",
      "('=', 4451)\n",
      "('century', 4438)\n",
      "('same', 4270)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-12-07 15:43:10\n",
      "-------------------------------------------\n",
      "('other', 15196)\n",
      "('first', 10617)\n",
      "('many', 9848)\n",
      "('new', 6289)\n",
      "('system', 5093)\n",
      "('american', 4824)\n",
      "('several', 4519)\n",
      "('century', 4493)\n",
      "('=', 4332)\n",
      "('same', 4260)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-12-07 15:43:15\n",
      "-------------------------------------------\n",
      "('other', 15091)\n",
      "('first', 10463)\n",
      "('many', 9703)\n",
      "('new', 6175)\n",
      "('system', 5102)\n",
      "('american', 4829)\n",
      "('several', 4523)\n",
      "('century', 4520)\n",
      "('=', 4369)\n",
      "('same', 4325)\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "\n",
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "lines = ssc.queueStream(rddQueue) #use rdd to simulate data stream\n",
    "\n",
    "# Do word-counting as before\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use transform() to access any rdd transformations not directly available in SparkStreaming\n",
    "topWords = wordCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "topWords.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(25)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda t:\n",
    "                                 (t[1][0] * t[1][1], t[0])) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:06:52 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('other', 7782), ('first', 5404), ('many', 4878), ('new', 3219), ('system', 2539)]\n",
      "refinery: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:07 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  76917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('other', 15486), ('first', 10815), ('many', 9773), ('new', 6272), ('system', 5063)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refinery: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:10 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  97338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('other', 23129), ('first', 16145), ('many', 14534), ('new', 9363), ('system', 7636)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refinery: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  114786\n",
      "[('other', 30805), ('first', 21524), ('many', 19348), ('new', 12514), ('system', 10174)]\n",
      "refinery: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:17 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  130393\n",
      "[('other', 38452), ('first', 26792), ('many', 24239), ('new', 15618), ('system', 12777)]\n",
      "refinery: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:18 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:23 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  144898\n",
      "[('other', 46151), ('first', 32041), ('many', 29054), ('new', 18732), ('system', 15440)]\n",
      "refinery: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:23 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:27 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  158013\n",
      "[('other', 53728), ('first', 37395), ('many', 33933), ('new', 21787), ('system', 17946)]\n",
      "refinery: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:28 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:32 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  170610\n",
      "[('other', 61347), ('first', 42658), ('many', 38902), ('new', 25021), ('system', 20533)]\n",
      "refinery: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:33 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:37 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  182330\n",
      "[('other', 68982), ('first', 47856), ('many', 43780), ('new', 28095), ('system', 23031)]\n",
      "refinery: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:38 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "21/12/07 16:07:42 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  193450\n",
      "[('other', 76438), ('first', 53121), ('many', 48605), ('new', 31196), ('system', 25635)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 16:07:43 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refinery: 40\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# 计算累积的word count\n",
    "\n",
    "# Stateful word count\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    #new Values: 新的stream中该单词的count\n",
    "    #runningCount : 旧有的统计中的单词的count\n",
    "    if runningCount is None: #原来的统计中该单词不存在\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.take(5))\n",
    "    print('refinery:', rdd.lookup('refinery')[0])\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MG algorithm for approximate word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "k = 10000\n",
    "threshold = 0\n",
    "total_decrement = 0\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    newValue = sum(newValues, runningCount) - threshold\n",
    "    return newValue if newValue > 0 else None\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "            \n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    global threshold, total_decrement \n",
    "    rdd.cache()\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.map(lambda x: (x[0], x[1], x[1]+total_decrement)).take(5))\n",
    "    lower_bound = rdd.lookup('refinery')\n",
    "    if len(lower_bound) > 0:\n",
    "        lower_bound = lower_bound[0]\n",
    "    else:\n",
    "        lower_bound = 0\n",
    "    print('refinery:', lower_bound, ',', lower_bound + total_decrement)\n",
    "    if rdd.count() > k:\n",
    "        threshold = rdd.zipWithIndex().map(lambda x: (x[1], x[0])).lookup(k)[0][1]\n",
    "    else:\n",
    "        threhold = 0\n",
    "    print(\"Next threshold = \", threshold)\n",
    "    total_decrement += threshold\n",
    "    rdd.unpersist()\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rdd = sc.parallelize([i, i, i, i, i])\n",
    "    rddQueue += [rdd]\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 3 seconds\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "nums = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Compute the sum over a sliding window of 9 seconds for every 3 seconds\n",
    "# slidingSum = nums.reduceByWindow(lambda x, y: x + y, None, 9, 3)\n",
    "slidingSum = nums.reduceByWindow(lambda x, y: x + y, lambda x, y: x - y, 9, 3) #add extra part minus front part\n",
    "\n",
    "slidingSum.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(24)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count using structured streaming: Complete mode vs update mode\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "word_counts = words.groupBy('word').count()\n",
    "\n",
    "# Start running the query \n",
    "query = word_counts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append mode with selection condition\n",
    "# Note: complete mode not supported if no aggregation\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "long_words = words.filter(length(words['word'])>=3)\n",
    "\n",
    "# Start running the query \n",
    "query = long_words\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n",
    "    words.word)\\\n",
    "    .count()\n",
    "\n",
    "# Start running the query \n",
    "query = windowedCounts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
